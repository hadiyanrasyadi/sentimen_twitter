{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import ELEProbDist, FreqDist\n",
    "from nltk import NaiveBayesClassifier\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pertama kita open datanya dengan pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data_latih2.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terus di praproses dengan kodingan hadiyan\n",
    "> \n",
    "* hilangkan html www, http dan lain lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "isi_dict = list(data[\"isi\"])\n",
    "clean_regular = []\n",
    "for regular in isi_dict:\n",
    "    regular = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))',' ',regular)\n",
    "    regular = re.sub('@[^\\s]+',' ',regular)\n",
    "    regular = re.sub('[\\s]+', ' ', regular)\n",
    "    regular = re.sub('#([^\\s]+)', ' ', regular)\n",
    "    regular = regular.strip('\\\"')\n",
    "    clean_regular.append(regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai normalisasi\n",
      "Donee menulis hasil normalisasi ^^\n",
      "mulai stemming..\n",
      "Donee menulis hasil Stemming ^^\n",
      "Clean Stopword\n",
      "Donee menulis hasil stopword removal ^^\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "#untuk membuat stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "with open('key_norm.csv') as filecsv, open('stopword_list_TALA.txt','r') as stp_file, open('hasil_normalisasi.txt','w') as hasil_norm, open('hasil_stemming.txt','w') as hasil_stem, open(\"hasil_stopword.txt\",'w') as hasil_stop, open(\"hasil_praproses.txt\",'w') as hasil_tweet:\n",
    "    read = csv.reader(filecsv, delimiter=',')\n",
    "    keys=[]\n",
    "    results=[]\n",
    "    clean_norm=[]\n",
    "\n",
    "    for row in read: # untuk mengubah csv ke dalam array\n",
    "        keys.append(row[1])\n",
    "        results.append(row[2])\n",
    "\n",
    "    gabung=dict(zip(keys,results)) #menggabungkan dua array\n",
    "\n",
    "    print('Mulai normalisasi')\n",
    "    clean_digit=[]\n",
    "    for s in clean_regular:\n",
    "        c_num = ''.join(i for i in s if not i.isdigit())\n",
    "        clean_digit.append(c_num)\n",
    "\n",
    "    for q in clean_digit: \n",
    "        temp = q.split()\n",
    "        # Ambil array Kata\n",
    "        for i,_ in enumerate(temp):\n",
    "            if temp[i] in gabung:\n",
    "                temp[i] = gabung[temp[i]]\n",
    "        temp2=' '.join(temp)\n",
    "        clean_norm.append(temp2)\n",
    "        \n",
    "#     ##### Menulis hasil Normalisasi ########\n",
    "#     print ('menulis hasil normalisasi')\n",
    "#     #menulis hasil tweet\n",
    "#     hasil_norm.write('[\\n')\n",
    "#     FNL= False\n",
    "#     for i in clean_norm:\n",
    "#         if FNL == True:\n",
    "#             hasil_norm.write(',\\n')\n",
    "#         FNL = True\n",
    "#         hasil_norm.write('\"('+str(i)+')\"')\n",
    "\n",
    "#     hasil_norm.write(']')\n",
    "\n",
    "    print('Donee menulis hasil normalisasi ^^')\n",
    "    #########################################\n",
    "\n",
    "\n",
    "    print('mulai stemming..')\n",
    "    # Proses stemming data\n",
    "    clean_stemmer = []\n",
    "    for csm in clean_norm:\n",
    "        clean = stemmer.stem(csm)\n",
    "        clean_stemmer.append(clean)\n",
    "\n",
    "#     ##### Menulis hasil Stemming ########\n",
    "#     print ('menulis hasil Stemming')\n",
    "#     #menulis hasil tweet\n",
    "#     hasil_stem.write('[\\n')\n",
    "#     FNL= False\n",
    "#     for i in clean_stemmer:\n",
    "#         if FNL == True:\n",
    "#             hasil_stem.write(',\\n')\n",
    "#         FNL = True\n",
    "#         hasil_stem.write('\"('+str(i)+')\"')\n",
    "\n",
    "#     hasil_stem.write(']')\n",
    "\n",
    "    print('Donee menulis hasil Stemming ^^')\n",
    "    #########################################\n",
    "    # Membersihkan Stopword\n",
    "    atp = [] #variabel menyimpan array dalam array\n",
    "    stp=[] #variabel mengubah atp menjadi 1 array saja\n",
    "    clean_stopword = []\n",
    "    #mengubah stopword yang txt ke bentuk array\n",
    "    for line in stp_file:\n",
    "        atp.append(line.strip().split('/n'))\n",
    "\n",
    "    stp=sum(atp,[])\n",
    "    print ('Clean Stopword')\n",
    "    for csw in clean_stemmer:\n",
    "        temp=csw.split() #membuat tokenize\n",
    "        clean_pnc = filter(lambda x: x not in string.punctuation,temp)\n",
    "        clean_sw = filter(lambda x: x not in stp,clean_pnc)\n",
    "        cc=' '.join(clean_sw) #menggabngkan tokenize\n",
    "        clean_stopword.append(cc)\n",
    "\n",
    "    # ##### Menulis hasil Normalisasi ########\n",
    "    # print ('menulis hasil stopword removal')\n",
    "    # #menulis hasil tweet\n",
    "    # hasil_stop.write('[\\n')\n",
    "    # FNL= False\n",
    "    # for i in clean_stopword:\n",
    "    #     if FNL == True:\n",
    "    #         hasil_stop.write(',\\n')\n",
    "    #     FNL = True\n",
    "    #     hasil_stop.write('\"('+str(i)+')\"')\n",
    "\n",
    "    # hasil_stop.write(']')\n",
    "\n",
    "    print('Donee menulis hasil stopword removal ^^')\n",
    "    #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_latih=list(data[\"sentimen\"])\n",
    "clean_tweet=[]\n",
    "for i in range(len(clean_stopword)):\n",
    "\tc = (clean_stopword[i],data_latih[i])\n",
    "\tclean_tweet.append(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tweets=[]\n",
    "for word,sentimen in clean_tweet:\n",
    "    words_filtered = [e.lower() for e in word.split()]\n",
    "    tweets.append((words_filtered, sentimen))\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "\n",
    "def get_word_features(wordlist):\t\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "\n",
    "\n",
    "##########################  Membuat train klasifier  #################################\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "def train(labeled_featuresets, estimator=ELEProbDist):\n",
    "    # Create the P(label) distribution\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    # Create the P(fval|label, fname) distribution\n",
    "    feature_probdist = {}\n",
    "    return NaiveBayesClassifier(label_probdist, feature_probdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "uji = pd.read_json(\"data_uji3.json\")\n",
    "uji_data= list(uji[\"isi\"])\n",
    "\n",
    "hasil_sentimen=[]\n",
    "for i in uji_data:\n",
    "    sentimen= classifier.classify(extract_features(i.split()))\n",
    "    hasil_sentimen.append(sentimen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_awal=uji[\"sentimen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_predik=pd.Series(hasil_sentimen)\n",
    "uji[\"hasil_prediksi\"]=hasil_predik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabel_hasil= open('tabel_hasil_baru.json', 'wb')\n",
    "uji.to_json(\"test1_baru.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "positif = 0\n",
    "positif_tp=0\n",
    "positif_fn=0\n",
    "negatif = 0\n",
    "negatif_tp=0\n",
    "negatif_fn=0\n",
    "netral = 0\n",
    "netral_tp=0\n",
    "netral_fn=0\n",
    "\n",
    "for i in range(len(data_awal)):\n",
    "    if data_awal[i] == \"netral\":\n",
    "        if data_awal[i] == hasil_sentimen[i]:\n",
    "            netral_tp = netral_tp + 1\n",
    "            continue\n",
    "        else:\n",
    "            netral_fn = netral_fn + 1\n",
    "            continue\n",
    "    elif data_awal[i] == \"positif\":\n",
    "        if data_awal[i] == hasil_sentimen[i]:\n",
    "            positif_tp = positif_tp + 1\n",
    "            continue\n",
    "        else:\n",
    "            positif_fn = positif_fn + 1\n",
    "            continue\n",
    "    else:\n",
    "        if data_awal[i] == hasil_sentimen[i]:\n",
    "            negatif_tp = negatif_tp + 1\n",
    "            continue\n",
    "        else:\n",
    "            negatif_fn = negatif_fn + 1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_pos=0\n",
    "pos_neg=0\n",
    "pos_net=0\n",
    "\n",
    "neg_neg=0\n",
    "neg_pos=0\n",
    "neg_net=0\n",
    "\n",
    "net_net=0\n",
    "net_pos=0\n",
    "net_neg=0\n",
    "\n",
    "\n",
    "for i in range(len(isi2_sentimen)):\n",
    "    if isi2_sentimen[i] == \"netral\":\n",
    "        if isi2_sentimen[i] == hasil_sentimen[i]:\n",
    "            net_net = net_net + 1\n",
    "            continue\n",
    "        elif hasil_sentimen[i] == \"positif\":\n",
    "            net_pos = net_pos + 1\n",
    "            continue\n",
    "        else:\n",
    "            net_neg = net_neg + 1\n",
    "    elif isi2_sentimen[i] == \"positif\":\n",
    "        if isi2_sentimen[i] == hasil_sentimen[i]:\n",
    "            pos_pos = pos_pos + 1\n",
    "            continue\n",
    "        elif hasil_sentimen[i] == \"negatif\":\n",
    "            pos_neg = pos_neg + 1\n",
    "            continue\n",
    "        else:\n",
    "            pos_net = pos_net + 1\n",
    "    else:\n",
    "        if isi2_sentimen[i] == hasil_sentimen[i]:\n",
    "            neg_neg = neg_neg + 1\n",
    "            continue\n",
    "        elif hasil_sentimen[i] == \"positif\":\n",
    "            neg_pos = neg_pos + 1\n",
    "            continue\n",
    "        else:\n",
    "            neg_net = neg_net + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil dari data yang sudah di praposes hasilnya list\n",
    "> \n",
    "* ubah kedalam bentuk series (pdSeries) yang ada nomernya kebawah\n",
    "* masukkan ke dataframe \"data\" dengan nama kolom \"isiClean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesClean = pd.Series(clean_stopword)\n",
    "data[\"isiClean\"] = seriesClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>akun</th>\n",
       "      <th>id_user</th>\n",
       "      <th>isi</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>isiClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JktMajuBersama</td>\n",
       "      <td>108503037</td>\n",
       "      <td>RT @NovNuraidah: @aniesbaswedan @sandiuno @Jkt...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>Tue Apr 04 14:07:14 +0000 2017</td>\n",
       "      <td>syariah kota hancur warung bahagia f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@JktMajuBersama</td>\n",
       "      <td>808673822404526083</td>\n",
       "      <td>RT @JktMajuBersama: Pesan dari Mas @aniesbaswe...</td>\n",
       "      <td>positif</td>\n",
       "      <td>Tue Apr 04 14:04:18 +0000 2017</td>\n",
       "      <td>pesan mas teman mahasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JktMajuBersama</td>\n",
       "      <td>2345017333</td>\n",
       "      <td>RT @JktMajuBersama: Press Conference Beredarny...</td>\n",
       "      <td>netral</td>\n",
       "      <td>Tue Apr 04 13:59:57 +0000 2017</td>\n",
       "      <td>press conference edar spanduk fitnah hadap ani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@JktMajuBersama</td>\n",
       "      <td>438861905</td>\n",
       "      <td>@sandiuno @JktMajuBersama #OKOCE https://t.co/...</td>\n",
       "      <td>netral</td>\n",
       "      <td>Tue Apr 04 13:58:41 +0000 2017</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@JktMajuBersama</td>\n",
       "      <td>328023910</td>\n",
       "      <td>RT @topelucky: Heuheuheuu... @JktMajuBersama i...</td>\n",
       "      <td>positif</td>\n",
       "      <td>Tue Apr 04 13:58:39 +0000 2017</td>\n",
       "      <td>heuheuheuu isi pemuda-pemudi dengar dengar cir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              akun             id_user  \\\n",
       "0  @JktMajuBersama           108503037   \n",
       "1  @JktMajuBersama  808673822404526083   \n",
       "2  @JktMajuBersama          2345017333   \n",
       "3  @JktMajuBersama           438861905   \n",
       "4  @JktMajuBersama           328023910   \n",
       "\n",
       "                                                 isi sentimen  \\\n",
       "0  RT @NovNuraidah: @aniesbaswedan @sandiuno @Jkt...  negatif   \n",
       "1  RT @JktMajuBersama: Pesan dari Mas @aniesbaswe...  positif   \n",
       "2  RT @JktMajuBersama: Press Conference Beredarny...   netral   \n",
       "3  @sandiuno @JktMajuBersama #OKOCE https://t.co/...   netral   \n",
       "4  RT @topelucky: Heuheuheuu... @JktMajuBersama i...  positif   \n",
       "\n",
       "                          tanggal  \\\n",
       "0  Tue Apr 04 14:07:14 +0000 2017   \n",
       "1  Tue Apr 04 14:04:18 +0000 2017   \n",
       "2  Tue Apr 04 13:59:57 +0000 2017   \n",
       "3  Tue Apr 04 13:58:41 +0000 2017   \n",
       "4  Tue Apr 04 13:58:39 +0000 2017   \n",
       "\n",
       "                                            isiClean  \n",
       "0               syariah kota hancur warung bahagia f  \n",
       "1                          pesan mas teman mahasiswa  \n",
       "2  press conference edar spanduk fitnah hadap ani...  \n",
       "3                                                     \n",
       "4  heuheuheuu isi pemuda-pemudi dengar dengar cir...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buat masing-masing list untuk masing-masing kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listPositif = []\n",
    "listNegatif = []\n",
    "listNetral = []\n",
    "for i in range(len(data)):\n",
    "    if(data[\"sentimen\"][i] == \"positif\"):\n",
    "        listPositif.append(data[\"isiClean\"][i])\n",
    "    elif(data[\"sentimen\"][i] == \"negatif\"):\n",
    "        listNegatif.append(data[\"isiClean\"][i])\n",
    "    else:\n",
    "        listNetral.append(data[\"isiClean\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesPositif = pd.Series(listPositif)\n",
    "seriesNegatif = pd.Series(listNegatif)\n",
    "seriesNetral = pd.Series(listNetral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesNegatif.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buat wordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cycler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-61e2d1ccfdf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1440\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1080\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseriesPositif\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\envs\\py35\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmplDeprecation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdedent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m from matplotlib.rcsetup import (defaultParams,\n\u001b[0m\u001b[0;32m    125\u001b[0m                                 \u001b[0mvalidate_backend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                                 cycler)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\envs\\py35\\lib\\site-packages\\matplotlib\\rcsetup.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Don't let the original cycler collide with our validating cycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCycler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mccycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# interactive_bk = ['gtk', 'gtkagg', 'gtkcairo', 'qt4agg',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'cycler'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(seriesPositif.astype(str)))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cycler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-bad1aaa0d414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1440\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1080\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseriesNegatif\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\envs\\py35\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmplDeprecation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdedent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m from matplotlib.rcsetup import (defaultParams,\n\u001b[0m\u001b[0;32m    125\u001b[0m                                 \u001b[0mvalidate_backend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                                 cycler)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\envs\\py35\\lib\\site-packages\\matplotlib\\rcsetup.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Don't let the original cycler collide with our validating cycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCycler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mccycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# interactive_bk = ['gtk', 'gtkagg', 'gtkcairo', 'qt4agg',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'cycler'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(seriesNegatif.astype(str)))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(seriesNetral.astype(str)))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_stopword.head(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
